{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101\n",
    "in Python 3.x, the core module for web scraping is split into several submodules:  \n",
    "**urllib** is a package that collects several modules for working with URLs:\n",
    " * **urllib.request** for opening and reading URLs\n",
    " * **urllib.error** containing the exceptions raised by urllib.request\n",
    " * **urllib.parse** for parsing URLs\n",
    " * **urllib.robotparser** for parsing robots.txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a html page link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = \"http://pythonscraping.com/pages/page1.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(URL)\n",
    "html.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen(URL) as f:\n",
    "    print(f.read(300).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anticipating http errors\n",
    "The web is messy, it pays to anticipate http errors and put in exceptions.  \n",
    "Two common things that can go wrong:\n",
    " * The page is not found on the server (404 Not Found)\n",
    " * The server is not found (500 Internal Server Error)\n",
    "\n",
    "For list of possible error codes, see: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL2 = 'https://www.womenwhocode.com/singapore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error as err\n",
    "try:\n",
    "    html = urlopen(URL2)\n",
    "except err.HTTPError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## request headers\n",
    "Some sites require basic header information, such as browser information, before providing page data, in order to deter robots.  \n",
    "This can be checked by looking at robots.txt of the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add header by building request\n",
    "req = r.Request(URL2)\n",
    "# Add User-Agent header value:\n",
    "req.add_header('User-Agent', 'Mozilla/5.0')\n",
    "html = r.urlopen(req)\n",
    "html.read(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use **OpenerDirector** to automatically adds a User-Agent header to every Request:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add headers by building an opener\n",
    "opener = r.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "opener.open(URL2).read(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Simpler method \n",
    "Using the requests library:\n",
    "\n",
    "    pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "html = requests.get(URL2)\n",
    "html.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "https://docs.python.org/3/library/urllib.request.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
